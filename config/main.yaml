# config/main.yaml
defaults:
  - dataset: re10k
  - optional dataset/view_sampler_dataset_specific_config: ${dataset/view_sampler}_${dataset}
  - model/autoencoder: kl_f8
  - model/encoder: epipolar
  - model/decoder: splatting_cuda
  - optional model/discriminator: null
  - experiment: speed3060 
  - _self_


model:
  supersampling_factor: 1
  encode_latents: false
  encoder:
    epipolar:
      num_samples: 196          
    epipolar_transformer:       
      num_context_views: 2
      num_samples: 484
      num_layers: 1
      num_heads: 4
      downscale: 4
      d_dot: 128
      d_mlp: 256
      self_attention:
        patch_size: 4
        num_octaves: 10
        num_layers: 1
        num_heads: 4
        d_token: 128
        d_dot: 128
        d_mlp: 256

freeze:
  autoencoder: false
  encoder: false
  decoder: false
  discriminator: false


hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S.%f}

wandb:
  project: latentsplat
  entity: placeholder
  name: placeholder
  mode: online
  activated: true

mode: train

dataset:
  overfit_to_scene: null

data_loader:
  # Avoid having to spin up new processes to print out visualizations.
  train:
    num_workers: 16
    persistent_workers: true
    batch_size: 2
    seed: 1234
  test:
    num_workers: 4
    persistent_workers: false
    batch_size: 1
    seed: 2345
  val:
    num_workers: 4
    persistent_workers: true
    batch_size: 1
    seed: 3456

optimizer:
  generator:
    name: Adam
    autoencoder_lr: 9.0e-6 # orig 4.5e-6 but doubled because of 2 context views
    scale_autoencoder_lr: true
    lr: 1.5e-4
    scale_lr: false
    autoencoder_kwargs:
      betas: [0.5, 0.9]
    scheduler:
      name: LinearLR
      kwargs:
        start_factor: 5.e-4
        total_iters: 2000
    gradient_clip_val: 0.5
  discriminator:
    name: Adam
    lr: 9.0e-6 # orig 4.5e-6 but doubled because of 2 context views
    scale_lr: true
    kwargs:
      betas: [0.5, 0.9]

checkpointing:
  load: null
  resume: false
  every_n_train_steps: 2500
  save_top_k: -1

train:
  depth_mode: null
  extended_visualization: false
  step_offset: 0
  video_interpolation: false
  video_wobble: false

test:
  output_path: outputs/test

seed: 111123


# datamodule:
#   batch_size: 4
#   num_workers: 8
#   pin_memory: true
#   base_resolution: 128
#   resolution_anneal: false

# datamodule:
#   batch_size: 12         
#   num_workers: 14        
#   pin_memory: true      
#   base_resolution: 128
#   resolution_anneal: false

# trainer:
#   # max_steps: 200_001
#   accelerator: gpu
#   devices: 1
#   precision: 16
#   max_steps: 500
#   val_check_interval: 200        
#   log_every_n_steps: 50         
#   strategy: ddp_graph            
#   accumulate_grad_batches: 8
#   logger:
#     _target_: pytorch_lightning.loggers.TensorBoardLogger
#     save_dir: outputs/tensorboard

# datamodule:
#   batch_size: 42
#   num_workers: 6
#   pin_memory: true
#   base_resolution: 128
#   resolution_anneal: true

# trainer:
#   accelerator: gpu
#   devices: 1
#   precision: bf16-mixed
#   max_steps: 300
#   val_check_interval: 5000
#   log_every_n_steps: 200
#   strategy: auto
#   accumulate_grad_batches: 1

datamodule:
  batch_size: 20          # 12 GB 安全值，256² 也不会爆
  num_workers: 4          # 单卡 4 核足够
  pin_memory: true
  base_resolution: 128
  resolution_anneal: true

trainer:
  accelerator: gpu
  devices: 1
  # precision: bf16-mixed   # 再省 15 % 显存
  precision: 16
  max_steps: 200
  val_check_interval: 5000
  log_every_n_steps: 200
  strategy: auto
  # accumulate_grad_batches: 1
  gradient_clip_val: null   # 手动优化，禁用裁剪

  logger:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: outputs/tensorboard

generate_gif: false
gif_interval: 25000  # 每 2.5 万步写一次，全程只写 8 张


loss:
  gaussian:
    nll:
      - name: l1
        weight: 1.0
        apply_after_step: 0
  target:
    render:
      image:          # ← 必须显式声明 image
        nll:
          - name: l1
            weight: 1.0
            apply_after_step: 0
