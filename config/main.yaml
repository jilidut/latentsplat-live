# config/main.yaml
defaults:
  - dataset: re10k
  - optional dataset/view_sampler_dataset_specific_config: ${dataset/view_sampler}_${dataset}
  - model/autoencoder: kl_f8
  - model/encoder: epipolar
  - model/decoder: splatting_cuda
  - optional model/discriminator: null
  - experiment: speed3060 
  - _self_

# datamodule:
#   batch_size: 4
#   num_workers: 8
#   pin_memory: true
#   base_resolution: 128
#   resolution_anneal: false

datamodule:
  batch_size: 2          # 先保平安，OOM 后再往上加
  num_workers: 4         # 之前 8→4
  pin_memory: false      # 之前 true→false
  base_resolution: 128
  resolution_anneal: false




model:
  supersampling_factor: 1
  encode_latents: false
  encoder:
    epipolar:
      num_samples: 196          
    epipolar_transformer:       
      num_context_views: 2
      num_samples: 484
      num_layers: 1
      num_heads: 4
      downscale: 4
      d_dot: 128
      d_mlp: 256
      self_attention:
        patch_size: 4
        num_octaves: 10
        num_layers: 1
        num_heads: 4
        d_token: 128
        d_dot: 128
        d_mlp: 256

freeze:
  autoencoder: false
  encoder: false
  decoder: false
  discriminator: false


hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S.%f}

wandb:
  project: latentsplat
  entity: placeholder
  name: placeholder
  mode: online
  activated: true

mode: train

dataset:
  overfit_to_scene: null

data_loader:
  # Avoid having to spin up new processes to print out visualizations.
  train:
    num_workers: 16
    persistent_workers: true
    batch_size: 2
    seed: 1234
  test:
    num_workers: 4
    persistent_workers: false
    batch_size: 1
    seed: 2345
  val:
    num_workers: 4
    persistent_workers: true
    batch_size: 1
    seed: 3456

optimizer:
  generator:
    name: Adam
    autoencoder_lr: 9.0e-6 # orig 4.5e-6 but doubled because of 2 context views
    scale_autoencoder_lr: true
    lr: 1.5e-4
    scale_lr: false
    autoencoder_kwargs:
      betas: [0.5, 0.9]
    scheduler:
      name: LinearLR
      kwargs:
        start_factor: 5.e-4
        total_iters: 2000
    gradient_clip_val: 0.5
  discriminator:
    name: Adam
    lr: 9.0e-6 # orig 4.5e-6 but doubled because of 2 context views
    scale_lr: true
    kwargs:
      betas: [0.5, 0.9]

checkpointing:
  load: null
  resume: false
  every_n_train_steps: 2500
  save_top_k: -1

train:
  depth_mode: null
  extended_visualization: false
  step_offset: 0
  video_interpolation: false
  video_wobble: false

test:
  output_path: outputs/test

seed: 111123

trainer:
  # max_steps: 200_001
  accelerator: cpu
  devices: 1
  precision: 32          # CPU 不支持 16-mixed
  max_steps: 100
  val_check_interval: 50
  log_every_n_steps: 10
  accumulate_grad_batches: 4
  logger:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: outputs/tensorboard


loss:
  gaussian:
    nll:
      - name: l1
        weight: 1.0
        apply_after_step: 0
  target:
    render:
      image:          # ← 必须显式声明 image
        nll:
          - name: l1
            weight: 1.0
            apply_after_step: 0

# callbacks:
#   - src.callbacks.generate_gif.GenerateGif:
#       ckpt_path: last
#       output_dir: outputs/render
#       fps: 12